{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the newsletter, following are the steps to building a RAG pipeline:\n",
    "\n",
    "Step 1: Take an inbound query and deconstruct it into relevant concepts  \n",
    "Step 2: Collect similar concepts from your data store  \n",
    "Step 3: Recombine these concepts with your original query to build a more relevant, authoritative answer.  \n",
    "Let's see how is the implementation in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Take an inbound query and deconstruct it into relevant concepts\n",
    "\n",
    "We can take a look at the `query_pipeline.py` file, specifically within the `configure_query_pipeline` function:\n",
    "\n",
    "```python\n",
    "def configure_query_pipeline(*, index: VectorStoreIndex, llm: OpenAI) -> QueryPipeline:\n",
    "    \"\"\"Configure and set up the query pipeline\"\"\"\n",
    "    text_qa_chat_template = ChatPromptTemplate.from_messages(_chat_template_messages)\n",
    "    query_pipeline = QueryPipeline()\n",
    "\n",
    "    retriever = index.as_retriever(similarity_top_k=_TOP_K_RETRIEVAL)\n",
    "    summarizer = TreeSummarize(\n",
    "        llm=llm, streaming=True, summary_template=text_qa_chat_template\n",
    "    )\n",
    "\n",
    "    query_pipeline.add_modules(\n",
    "        {\n",
    "            \"input\": InputComponent(),\n",
    "            \"retriever\": retriever,\n",
    "            \"summarizer\": summarizer,\n",
    "        }\n",
    "    )\n",
    "    query_pipeline.add_link(\"input\", \"retriever\")\n",
    "    query_pipeline.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")\n",
    "    query_pipeline.add_link(\"retriever\", \"summarizer\", dest_key=\"nodes\")\n",
    "\n",
    "    return query_pipeline\n",
    "```\n",
    "\n",
    "It takes the inbound query using InputComponent library and deconstructs it using QueryPipeline library.\n",
    "Those classes are provided by llama_index.core.query_pipeline (see llama_index.core.query_pipeline note for more detail)\n",
    "\n",
    "`InputComponent`is responsible for taking the raw query input and preparing it for further processing.\n",
    "`QueryPipeline` is to chain different components to create workflows for processing queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "\n",
    "A **retriever** in LlamaIndex is a component designed to fetch relevant information from a dataset based on a given query. It acts as a bridge between the raw data and the language model, ensuring that the most pertinent pieces of information are selected for further processing.\n",
    "\n",
    "#### Example Usage of a Retriever\n",
    "\n",
    "Imagine you have a collection of documents about various topics, and you want to find information related to \"machine learning.\" A retriever will search through the documents and return the most relevant ones.\n",
    "\n",
    "Here's a simple example:\n",
    "\n",
    "```python\n",
    "from llama_index import GPTSimpleVectorIndex\n",
    "\n",
    "# Load your data and create an index\n",
    "documents = [\"This is a document about machine learning.\", \"Another document discussing deep learning.\"]\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "# Use the retriever to find relevant documents\n",
    "query = \"machine learning\"\n",
    "retrieved_docs = retriever.retrieve(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n",
    "```\n",
    "\n",
    "In this example, the retriever searches the index for documents related to \"machine learning\" and returns them.\n",
    "\n",
    "### Summarizer\n",
    "\n",
    "A **summarizer** in LlamaIndex is a component that takes the retrieved information and generates a concise summary. It helps in distilling the essential points from the retrieved documents, making it easier to understand the key information.\n",
    "\n",
    "#### Example Usage of a Summarizer\n",
    "\n",
    "Continuing from the previous example, let's say you want to summarize the retrieved documents to get a brief overview of the content.\n",
    "\n",
    "Here's how you can use a summarizer:\n",
    "\n",
    "```python\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Create a summarizer\n",
    "summarizer = TreeSummarize(llm=OpenAI(model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "# Summarize the retrieved documents\n",
    "query_str = \"Summarize the information about machine learning\"\n",
    "text_chunks = [doc.text for doc in retrieved_docs]\n",
    "summary = summarizer.get_response(query_str, text_chunks)\n",
    "\n",
    "print(summary)\n",
    "```\n",
    "\n",
    "In this example, the summarizer takes the text chunks from the retrieved documents and generates a summary based on the query \"Summarize the information about machine learning.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
